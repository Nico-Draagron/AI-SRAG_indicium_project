# Databricks notebook source
# MAGIC %md
# MAGIC # üü´ Camada Bronze - Ingest√£o de Dados SRAG
# MAGIC 
# MAGIC **Projeto**: Sistema RAG para Monitoramento Epidemiol√≥gico - Indicium Healthcare PoC
# MAGIC 
# MAGIC **Objetivo**: Ingerir dados brutos de SRAG (2023-2025) do DATASUS para Delta Tables no Unity Catalog
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC ## üìã Responsabilidades da Camada Bronze
# MAGIC 
# MAGIC ‚úÖ **O que Bronze FAZ**:
# MAGIC - Ler CSVs originais sem modifica√ß√£o de dados
# MAGIC - Aplicar schema b√°sico (tipos corretos)
# MAGIC - Adicionar metadados t√©cnicos (`ANO_DADOS`, `_ingested_at`)
# MAGIC - Persistir em Delta Lake com versionamento
# MAGIC - Garantir idempot√™ncia (pode reprocessar)
# MAGIC 
# MAGIC ‚ùå **O que Bronze N√ÉO FAZ**:
# MAGIC - Filtrar registros (mesmo inv√°lidos)
# MAGIC - Tratar valores "Ignorado" (c√≥digo 9)
# MAGIC - Imputar missing values
# MAGIC - Aplicar regras de neg√≥cio
# MAGIC - Fazer agrega√ß√µes
# MAGIC 
# MAGIC üí° **Filosofia**: "Dados crus + rastreabilidade"
# MAGIC 
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîß 1. Setup e Configura√ß√£o

# COMMAND ----------

from pyspark.sql import functions as F
from pyspark.sql.types import *
from datetime import datetime
import json

# Configura√ß√£o de logging
print("=" * 70)
print("üü´ BRONZE LAYER - INGEST√ÉO DE DADOS SRAG")
print("=" * 70)
print(f"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"üîß Spark Version: {spark.version}")
print("=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìÅ 2. Configura√ß√£o de Paths (Unity Catalog)

# COMMAND ----------

# Configura√ß√£o do Unity Catalog
CATALOG = "workspace"
SCHEMA_BRONZE = "data_original"
VOLUME_RAW = "data_srag"    

VOLUME_PATH = f"/Volumes/{CATALOG}/{SCHEMA_BRONZE}/{VOLUME_RAW}"


# Paths baseados em Unity Catalog
VOLUME_PATH = f"/Volumes/{CATALOG}/{SCHEMA_BRONZE}/{VOLUME_RAW}"
TABLE_BRONZE = f"{CATALOG}.{SCHEMA_BRONZE}.bronze_srag_raw"

# Anos para processamento
YEARS = [2023, 2024, 2025]

print("üìÇ CONFIGURA√á√ÉO DE PATHS:")
print(f"  ‚Ä¢ Cat√°logo: {CATALOG}")
print(f"  ‚Ä¢ Schema: {SCHEMA_BRONZE}")
print(f"  ‚Ä¢ Volume: {VOLUME_RAW}")
print(f"  ‚Ä¢ Path CSVs: {VOLUME_PATH}")
print(f"  ‚Ä¢ Tabela destino: {TABLE_BRONZE}")
print(f"  ‚Ä¢ Anos: {YEARS}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üì• 3. Verifica√ß√£o de Arquivos

# COMMAND ----------

# Listar arquivos dispon√≠veis no volume
try:
    files = dbutils.fs.ls(VOLUME_PATH)
    csv_files = [f for f in files if f.name.endswith('.csv')]
    
    print(f"\n‚úÖ {len(csv_files)} arquivo(s) CSV encontrado(s):\n")
    for f in csv_files:
        size_mb = f.size / (1024 * 1024)
        print(f"  üìÑ {f.name:<20} ({size_mb:,.2f} MB)")
    
    if len(csv_files) == 0:
        raise FileNotFoundError("Nenhum arquivo CSV encontrado no volume!")
    
except Exception as e:
    print(f"‚ùå ERRO ao acessar volume: {str(e)}")
    print("\nüí° A√á√ïES NECESS√ÅRIAS:")
    print(f"  1. Verificar se o volume existe: {VOLUME_PATH}")
    print(f"  2. Fazer upload dos CSVs (INFLUD23.csv, INFLUD24.csv, INFLUD25.csv)")
    print(f"  3. Verificar permiss√µes no Unity Catalog")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç 4. Fun√ß√£o de Ingest√£o por Ano

# COMMAND ----------

def ingest_year_data(year: int, volume_path: str, validate_only: bool = False):
    """
    Ingere dados de SRAG de um ano espec√≠fico
    
    Args:
        year: Ano dos dados (2023, 2024, ou 2025)
        volume_path: Path do volume com os CSVs
        validate_only: Se True, apenas valida sem carregar dados completos
    
    Returns:
        DataFrame Spark ou dict com estat√≠sticas (se validate_only=True)
    """
    
    # Identificar arquivo do ano
    year_suffix = str(year)[2:]  # 2023 -> 23
    file_pattern = f"INFLUD{year_suffix}-*.csv"
    file_path = f"{volume_path}/{file_pattern}"

    
    print(f"\n{'=' * 70}")
    print(f"üì• PROCESSANDO ANO {year}")
    print(f"{'=' * 70}")
    print(f"üìÇ Arquivo: {file_pattern}")
    
    try:
        # Leitura com configura√ß√µes espec√≠ficas para SRAG/DATASUS
        df = spark.read \
            .option("header", "true") \
            .option("sep", ";") \
            .option("encoding", "ISO-8859-1") \
            .option("inferSchema", "false") \
            .option("quote", '"') \
            .option("escape", '"') \
            .option("multiLine", "true") \
            .option("mode", "PERMISSIVE") \
            .csv(file_path)
        
        # Estat√≠sticas b√°sicas
        if validate_only:
            # Apenas contagem (sem materializar DataFrame completo)
            row_count = df.count()
            col_count = len(df.columns)
            
            stats = {
                'year': year,
                'file': file_pattern,
                'rows': row_count,
                'columns': col_count,
                'status': 'OK'
            }
            
            print(f"  ‚úÖ Registros: {row_count:,}")
            print(f"  ‚úÖ Colunas: {col_count}")
            
            return stats
        
        else:
            # Carregar dados completos
            row_count = df.count()
            col_count = len(df.columns)
            
            print(f"  ‚úÖ Registros: {row_count:,}")
            print(f"  ‚úÖ Colunas: {col_count}")
            
            # Adicionar metadados t√©cnicos (enriquecimento m√≠nimo permitido no Bronze)
            df_enriched = df \
                .withColumn("ANO_DADOS", F.lit(year)) \
                .withColumn("_ingested_at", F.current_timestamp()) \
                .withColumn("_source_file", F.lit(file_pattern))
            
            print(f"  ‚úÖ Metadados adicionados: ANO_DADOS, _ingested_at, _source_file")
            
            return df_enriched
    
    except Exception as e:
        print(f"  ‚ùå ERRO ao processar {year}: {str(e)}")
        raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úîÔ∏è 5. Valida√ß√£o Pr√©via (Dry Run)

# COMMAND ----------

print("\nüîç VALIDA√á√ÉO PR√âVIA (Dry Run):")
print("Verificando todos os arquivos antes de ingest√£o completa...\n")

validation_results = []

for year in YEARS:
    try:
        stats = ingest_year_data(year, VOLUME_PATH, validate_only=True)
        validation_results.append(stats)
    except Exception as e:
        validation_results.append({
            'year': year,
            'status': 'ERRO',
            'error': str(e)
        })

# Resumo da valida√ß√£o
print(f"\n{'=' * 70}")
print("üìä RESUMO DA VALIDA√á√ÉO")
print(f"{'=' * 70}")

total_rows = 0
all_ok = True

for result in validation_results:
    if result['status'] == 'OK':
        print(f"  ‚úÖ {result['year']}: {result['rows']:,} registros | {result['columns']} colunas")
        total_rows += result['rows']
    else:
        print(f"  ‚ùå {result['year']}: FALHOU - {result.get('error', 'Erro desconhecido')}")
        all_ok = False

print(f"\n  üìä TOTAL ESTIMADO: {total_rows:,} registros")

if not all_ok:
    raise Exception("‚ùå Valida√ß√£o falhou. Corrija os erros antes de prosseguir.")

print("\n‚úÖ Valida√ß√£o conclu√≠da com sucesso! Prosseguindo para ingest√£o completa...")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üì¶ 6. Ingest√£o Completa

# COMMAND ----------

print("\nüöÄ INICIANDO INGEST√ÉO COMPLETA...")

dataframes = {}

for year in YEARS:
    df = ingest_year_data(year, VOLUME_PATH, validate_only=False)
    dataframes[year] = df
    

print(f"\n‚úÖ {len(dataframes)} DataFrames carregados em mem√≥ria")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîó 7. Uni√£o dos Anos

# COMMAND ----------

print("\nüîó UNINDO DADOS DE TODOS OS ANOS...")

# Uni√£o com allowMissingColumns (SRAG muda schema entre anos)
df_bronze = dataframes[YEARS[0]]

for year in YEARS[1:]:
    df_bronze = df_bronze.unionByName(
        dataframes[year], 
        allowMissingColumns=True
    )

# Estat√≠sticas finais
total_rows = df_bronze.count()
total_cols = len(df_bronze.columns)

print(f"\n{'=' * 70}")
print("üìä DATASET BRONZE CONSOLIDADO")
print(f"{'=' * 70}")
print(f"  üìà Total de registros: {total_rows:,}")
print(f"  üìã Total de colunas: {total_cols}")
print(f"  üìÖ Per√≠odo: {min(YEARS)} - {max(YEARS)}")
print(f"{'=' * 70}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç 8. Inspe√ß√£o Estrutural

# COMMAND ----------

print("\nüîç INSPE√á√ÉO ESTRUTURAL DO DATASET:")

# Schema resumido (primeiras 20 colunas)
print("\nüìã SCHEMA (primeiras 20 colunas):")
for field in df_bronze.schema.fields[:20]:
    print(f"  ‚Ä¢ {field.name:<25} ({field.dataType})")

if total_cols > 20:
    print(f"  ... e mais {total_cols - 20} colunas")

# COMMAND ----------

# Distribui√ß√£o por ano
print("\nüìÖ DISTRIBUI√á√ÉO POR ANO:")
df_bronze.groupBy("ANO_DADOS") \
    .count() \
    .orderBy("ANO_DADOS") \
    .show()

# COMMAND ----------

# Amostra dos dados
print("\nüëÄ AMOSTRA DE DADOS (5 primeiras linhas):")
print("Exibindo apenas colunas essenciais para verifica√ß√£o...")

essential_cols = [
    'DT_NOTIFIC', 'DT_SIN_PRI', 'SEM_PRI', 'CS_SEXO', 
    'NU_IDADE_N', 'SG_UF', 'HOSPITAL', 'UTI', 'EVOLUCAO',
    'ANO_DADOS', '_ingested_at'
]

# Filtrar apenas colunas que existem
display_cols = [col for col in essential_cols if col in df_bronze.columns]

display(df_bronze.select(display_cols).limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## üíæ 9. Persist√™ncia em Delta Lake

# COMMAND ----------

print("\nüíæ SALVANDO DADOS NA CAMADA BRONZE...")
print(f"üéØ Tabela destino: {TABLE_BRONZE}")

# Configura√ß√£o de escrita
write_config = {
    'mode': 'overwrite',  # Sobrescrever para garantir idempot√™ncia
    'format': 'delta',
    'overwriteSchema': True  # Permitir schema evolution
}

print(f"\n‚öôÔ∏è CONFIGURA√á√ÉO DE ESCRITA:")
for key, value in write_config.items():
    print(f"  ‚Ä¢ {key}: {value}")

# Escrever Delta Table
try:
    df_bronze.write \
        .mode(write_config['mode']) \
        .format(write_config['format']) \
        .option("overwriteSchema", write_config['overwriteSchema']) \
        .saveAsTable(TABLE_BRONZE)
    
    print(f"\n‚úÖ Tabela {TABLE_BRONZE} criada com sucesso!")
    
except Exception as e:
    print(f"\n‚ùå ERRO ao salvar tabela: {str(e)}")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ 10. Valida√ß√£o da Tabela Criada

# COMMAND ----------

print("\n‚úîÔ∏è VALIDANDO TABELA CRIADA...")

# Ler tabela rec√©m-criada
df_validation = spark.table(TABLE_BRONZE)

# Verifica√ß√µes
validation_checks = {
    'row_count_match': df_validation.count() == total_rows,
    'columns_count': len(df_validation.columns) == total_cols,
    'has_metadata': all(col in df_validation.columns for col in ['ANO_DADOS', '_ingested_at', '_source_file']),
    'all_years_present': df_validation.select('ANO_DADOS').distinct().count() == len(YEARS)
}

print(f"\n{'=' * 70}")
print("üîç VERIFICA√á√ïES DE INTEGRIDADE")
print(f"{'=' * 70}")

all_passed = True
for check, result in validation_checks.items():
    status = "‚úÖ PASS" if result else "‚ùå FAIL"
    print(f"  {status} - {check}")
    if not result:
        all_passed = False

if not all_passed:
    raise Exception("‚ùå Valida√ß√£o da tabela falhou!")

print(f"\n‚úÖ Todas as verifica√ß√µes passaram!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä 11. Metadados da Tabela

# COMMAND ----------

# Informa√ß√µes da tabela Delta
print("\nüìä METADADOS DA TABELA DELTA:")

# Descri√ß√£o
spark.sql(f"DESCRIBE EXTENDED {TABLE_BRONZE}").show(50, truncate=False)

# COMMAND ----------

# Hist√≥rico Delta (time travel)
print("\nüïê HIST√ìRICO DELTA (Time Travel):")

try:
    history = spark.sql(f"DESCRIBE HISTORY {TABLE_BRONZE}")
    display(history.select('version', 'timestamp', 'operation', 'operationMetrics'))
except Exception as e:
    print(f"‚ÑπÔ∏è Hist√≥rico n√£o dispon√≠vel: {str(e)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìà 12. Estat√≠sticas e M√©tricas

# COMMAND ----------

# Estat√≠sticas de ingest√£o
ingestion_stats = {
    'timestamp': datetime.now().isoformat(),
    'catalog': CATALOG,
    'schema': SCHEMA_BRONZE,
    'table': TABLE_BRONZE.split('.')[-1],
    'total_rows': total_rows,
    'total_columns': total_cols,
    'years_processed': YEARS,
    'source_files': [f"INFLUD{str(y)[2:]}.csv" for y in YEARS],
    'validation_passed': all_passed
}

print("\nüìà ESTAT√çSTICAS DE INGEST√ÉO:")
print(json.dumps(ingestion_stats, indent=2))

# Salvar estat√≠sticas (opcional)
stats_path = f"/Volumes/{CATALOG}/{SCHEMA_BRONZE}/logs/ingestion_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

try:
    dbutils.fs.mkdirs(f"/Volumes/{CATALOG}/{SCHEMA_BRONZE}/logs")
    dbutils.fs.put(stats_path, json.dumps(ingestion_stats, indent=2), overwrite=True)
    print(f"\nüíæ Estat√≠sticas salvas em: {stats_path}")
except Exception as e:
    print(f"\n‚ÑπÔ∏è N√£o foi poss√≠vel salvar estat√≠sticas: {str(e)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üéØ 13. Conclus√£o e Pr√≥ximos Passos

# COMMAND ----------

print("\n" + "=" * 70)
print("üéâ INGEST√ÉO BRONZE CONCLU√çDA COM SUCESSO!")
print("=" * 70)

print(f"\nüìä RESUMO:")
print(f"  ‚Ä¢ Tabela criada: {TABLE_BRONZE}")
print(f"  ‚Ä¢ Registros totais: {total_rows:,}")
print(f"  ‚Ä¢ Colunas: {total_cols}")
print(f"  ‚Ä¢ Anos: {', '.join(map(str, YEARS))}")
print(f"  ‚Ä¢ Formato: Delta Lake")
print(f"  ‚Ä¢ Schema evolution: Habilitado")
print(f"  ‚Ä¢ Time travel: Dispon√≠vel")

print(f"\n‚úÖ QUALIDADE DOS DADOS:")
print(f"  ‚Ä¢ Todas as valida√ß√µes passaram")
print(f"  ‚Ä¢ Metadados t√©cnicos adicionados")
print(f"  ‚Ä¢ Tabela pronta para pr√≥xima camada")

print(f"\nüöÄ PR√ìXIMOS PASSOS:")
print(f"  1. Notebook 02: Valida√ß√£o de Qualidade (Data Quality Checks)")
print(f"  2. Notebook 03: EDA Profissional (An√°lise Explorat√≥ria)")
print(f"  3. Notebook 04: Camada Silver (Limpeza e Transforma√ß√£o)")

print("\n" + "=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC 
# MAGIC ## üìö Documenta√ß√£o e Refer√™ncias
# MAGIC 
# MAGIC ### Arquitetura Medallion
# MAGIC 
# MAGIC ```
# MAGIC üü´ BRONZE (voc√™ est√° aqui)
# MAGIC  ‚Üì
# MAGIC üîç VALIDATION
# MAGIC  ‚Üì
# MAGIC üìä EDA
# MAGIC  ‚Üì
# MAGIC ü•à SILVER
# MAGIC  ‚Üì
# MAGIC ü•á GOLD
# MAGIC  ‚Üì
# MAGIC ü§ñ RAG
# MAGIC ```
# MAGIC 
# MAGIC ### Decis√µes Arquiteturais
# MAGIC 
# MAGIC 1. **Metadados no Bronze**: `ANO_DADOS`, `_ingested_at`, `_source_file`
# MAGIC    - Justificativa: Enriquecimento t√©cnico m√≠nimo para rastreabilidade
# MAGIC    - N√£o viola princ√≠pio "Bronze = dados crus" (s√£o metadados, n√£o transforma√ß√µes)
# MAGIC 
# MAGIC 2. **`allowMissingColumns=True`**
# MAGIC    - SRAG muda schema entre anos (ex: campos COVID em 2023+)
# MAGIC    - Union precisa ser flex√≠vel
# MAGIC 
# MAGIC 3. **`overwriteSchema=True`**
# MAGIC    - Permite reprocessamento com schema evolution
# MAGIC    - Essencial para manuten√ß√£o do pipeline
# MAGIC 
# MAGIC ### Fonte dos Dados
# MAGIC 
# MAGIC - **Sistema**: SIVEP-Gripe (DATASUS)
# MAGIC - **Per√≠odo**: 2023-2025
# MAGIC - **Formato original**: CSV (separador `;`, encoding ISO-8859-1)
# MAGIC - **Dicion√°rio**: `dicionario_de_dados_SRAG_hospitalizado_2019.pdf`
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC **Desenvolvido para**: Indicium Healthcare PoC - AI Engineer Certification
# MAGIC 
# MAGIC **Pr√≥ximo notebook**: `02_Data_Quality_Validation.py`